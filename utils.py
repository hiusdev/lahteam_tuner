"""
LahTeam Musubi Tuner - Helper Functions
Ch·ªâ ch·ª©a c√°c function h·ªó tr·ª£ c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng.

Usage trong Colab:
    !git clone https://github.com/LahTeam/colab_musubi_tuner
    import sys
    sys.path.append('/content/colab_musubi_tuner')
    from lahteam_tuner.utils import download_component, patch_logger_files
"""

import os
import re
from typing import Optional, Dict, List, Any


# ==============================================================================
# DOWNLOAD HELPERS
# ==============================================================================

def download_component(
    component_name: str, 
    base_dir: str, 
    config: dict,
    hf_token: Optional[str] = None
) -> Optional[str]:
    """
    T·∫£i m·ªôt th√†nh ph·∫ßn m√¥ h√¨nh (file ho·∫∑c folder) t·ª´ HuggingFace.
    
    Args:
        component_name: T√™n th√†nh ph·∫ßn (VAE, Text Encoder, DiT)
        base_dir: Th∆∞ m·ª•c g·ªëc ƒë·ªÉ l∆∞u
        config: Dict ch·ª©a {type, repo_id, filename/folder, subfolder, local_folder}
        hf_token: HuggingFace token (optional)
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file/folder ƒë√£ t·∫£i, ho·∫∑c None n·∫øu l·ªói
    
    Example:
        config = {
            "type": "file",
            "repo_id": "Comfy-Org/z_image_turbo",
            "filename": "ae.safetensors",
            "subfolder": "split_files/vae"
        }
        path = download_component("VAE", "/content/models", config)
    """
    from huggingface_hub import hf_hub_download, snapshot_download
    
    download_type = config.get("type", "file")
    repo_id = config["repo_id"]
    
    if download_type == "folder":
        folder = config.get("folder", "")
        local_folder = config.get("local_folder", folder)
        full_path = os.path.join(base_dir, local_folder)
        
        if os.path.exists(full_path) and os.listdir(full_path):
            print(f"‚úÖ [{component_name.upper()}] Folder ƒë√£ t·ªìn t·∫°i: {local_folder}")
            return full_path
        
        print(f"‚è≥ [{component_name.upper()}] ƒêang t·∫£i folder '{folder}' t·ª´ {repo_id}...")
        try:
            os.makedirs(os.path.dirname(full_path), exist_ok=True)
            snapshot_download(
                repo_id=repo_id,
                allow_patterns=f"{folder}/*" if folder else None,
                local_dir=base_dir,
                local_dir_use_symlinks=False,
                token=hf_token
            )
            print(f"   -> T·∫£i folder th√†nh c√¥ng!")
            return full_path
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i [{component_name.upper()}]: {e}")
            return None
    else:
        subfolder = config.get('subfolder', '')
        filename = config['filename']
        full_path = os.path.join(base_dir, subfolder, filename) if subfolder else os.path.join(base_dir, filename)

        if os.path.exists(full_path):
            print(f"‚úÖ [{component_name.upper()}] File ƒë√£ t·ªìn t·∫°i: {filename}")
            return full_path

        print(f"‚è≥ [{component_name.upper()}] ƒêang t·∫£i '{filename}'...")
        os.makedirs(os.path.dirname(full_path) if os.path.dirname(full_path) else base_dir, exist_ok=True)
        
        try:
            hf_hub_download(
                repo_id=repo_id,
                filename=f"{subfolder}/{filename}" if subfolder else filename,
                local_dir=base_dir,
                token=hf_token
            )
            print(f"   -> T·∫£i th√†nh c√¥ng!")
            return full_path
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i [{component_name.upper()}]: {e}")
            return None


def download_and_unzip(
    url: str, 
    unzip_dir: str, 
    zip_filename: str = "temp.zip",
    hf_token: Optional[str] = None
) -> bool:
    """
    T·∫£i file zip t·ª´ URL v√† gi·∫£i n√©n.
    
    Args:
        url: URL file zip
        unzip_dir: Th∆∞ m·ª•c gi·∫£i n√©n
        zip_filename: T√™n file zip t·∫°m
        hf_token: Token cho private repos
    
    Returns:
        bool: True n·∫øu th√†nh c√¥ng
    """
    import subprocess
    import zipfile
    
    if not url:
        return False
    
    if os.path.exists(unzip_dir) and any(os.scandir(unzip_dir)):
        print(f"‚úÖ Th∆∞ m·ª•c '{unzip_dir}' ƒë√£ c√≥ d·ªØ li·ªáu. B·ªè qua.")
        return True
    
    print(f"‚è≥ ƒêang t·∫£i t·ª´: {url}")
    zip_path = f"/content/{zip_filename}"
    header = f'"Authorization: Bearer {hf_token}"' if hf_token else ''
    
    cmd = f'aria2c --console-log-level=error -c -x 16 -k 1M -s 16 --header={header} -d /content -o {zip_filename} "{url}"'
    subprocess.run(cmd, shell=True)
    
    if os.path.exists(zip_path):
        print(f"üì¶ ƒêang gi·∫£i n√©n...")
        os.makedirs(unzip_dir, exist_ok=True)
        with zipfile.ZipFile(zip_path, 'r') as zf:
            zf.extractall(unzip_dir)
        os.remove(zip_path)
        print(f"‚úÖ Gi·∫£i n√©n ho√†n t·∫•t!")
        return True
    
    print(f"‚ùå T·∫£i th·∫•t b·∫°i!")
    return False


# ==============================================================================
# LOGGER PATCH HELPERS
# ==============================================================================

def replace_logger_with_print(file_path: str) -> bool:
    """
    Thay th·∫ø logger.info/warning/error/debug th√†nh print().
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file Python
    
    Returns:
        bool: True n·∫øu c√≥ thay ƒë·ªïi
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    pattern = r'\blogger\.(info|warning|error|debug)\s*\((.*?)\)'
    replaced = re.sub(pattern, r'print(\2)', content, flags=re.DOTALL)
    
    if replaced != content:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(replaced)
        return True
    return False


def remove_setup_logging_calls(file_path: str) -> bool:
    """
    X√≥a c√°c l·ªùi g·ªçi setup_logging().
    
    Args:
        file_path: ƒê∆∞·ªùng d·∫´n file Python
    
    Returns:
        bool: True n·∫øu c√≥ thay ƒë·ªïi
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    new_lines = []
    removed = False
    
    for line in lines:
        stripped = line.strip()
        if stripped in ["setup_logging()", "setup_logging(args, reset=True)"]:
            removed = True
            continue
        new_lines.append(line)
    
    if removed:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.writelines(new_lines)
    return removed


def patch_logger_files(repo_dir: str, verbose: bool = False):
    """
    Patch t·∫•t c·∫£ file Python trong th∆∞ m·ª•c: thay logger th√†nh print, x√≥a setup_logging.
    
    Args:
        repo_dir: Th∆∞ m·ª•c g·ªëc c·∫ßn patch
        verbose: In chi ti·∫øt c√°c file ƒë√£ s·ª≠a
    """
    count = 0
    for root, _, files in os.walk(repo_dir):
        for file in files:
            if file.endswith('.py'):
                path = os.path.join(root, file)
                try:
                    changed = replace_logger_with_print(path)
                    changed2 = remove_setup_logging_calls(path)
                    if (changed or changed2) and verbose:
                        print(f"‚úÖ Patched: {path}")
                        count += 1
                except Exception as e:
                    if verbose:
                        print(f"‚ö†Ô∏è Error: {path}: {e}")
    print(f"üîß ƒê√£ patch {count} files")


# ==============================================================================
# DATASET CONFIG HELPERS
# ==============================================================================

def find_images_in_folder(path: str) -> bool:
    """Ki·ªÉm tra folder c√≥ ch·ª©a ·∫£nh kh√¥ng."""
    import glob
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        if glob.glob(os.path.join(path, ext)):
            return True
    return False


def get_repeats_from_folder_name(folder_name: str, default: int = 10) -> int:
    """
    L·∫•y s·ªë repeats t·ª´ t√™n folder (format: 10_name).
    
    Args:
        folder_name: T√™n folder
        default: Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t√¨m th·∫•y
    
    Returns:
        int: S·ªë repeats
    """
    parts = folder_name.split('_')
    if len(parts) > 1 and parts[0].isdigit():
        return int(parts[0])
    return default


# ==============================================================================
# TRAIN COMMAND HELPERS
# ==============================================================================

def build_train_args(config: Dict[str, Any]) -> str:
    """
    Build command line arguments t·ª´ dict config.
    
    Args:
        config: Dict ch·ª©a c√°c tham s·ªë training
    
    Returns:
        str: Chu·ªói arguments
    
    Example:
        config = {"dit": "/path/dit", "learning_rate": 1e-4, "fp8_base": True}
        args = build_train_args(config)
        # -> '--dit="/path/dit" --learning_rate="0.0001" --fp8_base'
    """
    args = ""
    for k, v in config.items():
        if v is None or v is False:
            continue
        if isinstance(v, list):
            for item in v:
                args += f' --{k}="{item}"'
        elif isinstance(v, bool) and v is True:
            args += f" --{k}"
        else:
            args += f' --{k}="{v}"'
    return args


def get_script_prefix(model_type: str) -> str:
    """
    L·∫•y prefix cho script d·ª±a tr√™n model_type.
    
    Args:
        model_type: Lo·∫°i model
    
    Returns:
        str: Prefix (flux_2, zimage, qwen_image)
    """
    if model_type.startswith("flux2_"):
        return "flux_2"
    elif model_type == "z_image_turbo":
        return "zimage"
    else:
        return "qwen_image"


def get_network_module(model_type: str) -> str:
    """
    L·∫•y network module d·ª±a tr√™n model_type.
    
    Args:
        model_type: Lo·∫°i model
    
    Returns:
        str: Network module path
    """
    if model_type.startswith("flux2_"):
        return "networks.lora_flux_2"
    elif model_type == "z_image_turbo":
        return "networks.lora_zimage"
    else:
        return "networks.lora_qwen_image"


# ==============================================================================
# MODEL INFO HELPERS
# ==============================================================================

# FLUX.2 Version mapping
FLUX2_VERSION_MAP = {
    "flux2_dev": "dev",
    "flux2_klein_4b": "klein-4b",
    "flux2_klein_base_4b": "klein-base-4b",
    "flux2_klein_9b": "klein-9b",
    "flux2_klein_base_9b": "klein-base-9b"
}

# Max blocks_to_swap cho FLUX.2
FLUX2_BLOCKS_LIMIT = {
    "flux2_dev": 29,
    "flux2_klein_4b": 13,
    "flux2_klein_base_4b": 13,
    "flux2_klein_9b": 16,
    "flux2_klein_base_9b": 16,
}


def get_flux2_version(model_type: str) -> str:
    """L·∫•y FLUX.2 version string t·ª´ model_type."""
    return FLUX2_VERSION_MAP.get(model_type, "")


def get_max_blocks_to_swap(model_type: str) -> int:
    """L·∫•y gi·ªõi h·∫°n blocks_to_swap cho model_type."""
    return FLUX2_BLOCKS_LIMIT.get(model_type, 36)


def is_flux2_model(model_type: str) -> bool:
    """Ki·ªÉm tra c√≥ ph·∫£i FLUX.2 model kh√¥ng."""
    return model_type.startswith("flux2_")


def is_edit_model(model_type: str) -> bool:
    """Ki·ªÉm tra c√≥ ph·∫£i Edit model kh√¥ng."""
    return model_type in ["qwen_image_edit", "qwen_image_edit_2509"]


def get_train_script(model_type: str) -> str:
    """L·∫•y t√™n file script training."""
    if model_type.startswith("flux2_"):
        return "flux_2_train_network.py"
    elif model_type == "z_image_turbo":
        return "zimage_train_network.py"
    else:
        return "qwen_image_train_network.py"


def get_cache_script_prefix(model_type: str) -> str:
    """L·∫•y prefix cho cache scripts (latents & text encoder)."""
    return get_script_prefix(model_type)


# ==============================================================================
# DATASET CONFIG HELPERS (TOML)
# ==============================================================================

def create_dataset_config(
    data_dir: str,
    control_dir: Optional[str] = None,
    cache_dir: Optional[str] = None,
    config_dir: str = "/content/fine_tune/config",
    resolution: tuple = (960, 544),
    control_resolution: Optional[tuple] = None,
    batch_size: int = 1,
    default_repeats: int = 1,
    caption_extension: str = ".txt",
    enable_bucket: bool = True,
    bucket_no_upscale: bool = False,
    no_resize_control: bool = False,
    model_type: str = "qwen_image"
) -> str:
    """
    T·∫°o file dataset_config.toml theo format musubi-tuner ch√≠nh th·ª©c.
    
    Args:
        data_dir: Th∆∞ m·ª•c ch·ª©a ·∫£nh (c√≥ th·ªÉ c√≥ subfolders 10_name)
        control_dir: Th∆∞ m·ª•c control images (cho FLUX.2/Qwen Edit)
        cache_dir: Th∆∞ m·ª•c cache (m·∫∑c ƒë·ªãnh = data_dir)
        config_dir: Th∆∞ m·ª•c l∆∞u file config
        resolution: (width, height) m·∫∑c ƒë·ªãnh [960, 544]
        control_resolution: Resolution cho control images
                           - Qwen Edit: ƒë·ªÅ ngh·ªã [1024, 1024]
                           - FLUX.2 1 control: [2024, 2024]
                           - FLUX.2 multi control: [1024, 1024]
        batch_size: Batch size, m·∫∑c ƒë·ªãnh 1
        default_repeats: S·ªë repeats m·∫∑c ƒë·ªãnh, m·∫∑c ƒë·ªãnh 1
        caption_extension: Extension caption files
        enable_bucket: B·∫≠t bucketing
        bucket_no_upscale: Kh√¥ng upscale khi bucket
        no_resize_control: Kh√¥ng resize control images (d√πng cho FLUX.2)
        model_type: Lo·∫°i model ƒë·ªÉ auto-detect settings
    
    Returns:
        str: ƒê∆∞·ªùng d·∫´n file config ƒë√£ t·∫°o
    
    Example:
        # Qwen Edit
        path = create_dataset_config(
            data_dir="/content/data/output",
            control_dir="/content/data/input",
            control_resolution=(1024, 1024),
            model_type="qwen_image_edit"
        )
        
        # FLUX.2
        path = create_dataset_config(
            data_dir="/content/data/output",
            control_dir="/content/data/input",
            control_resolution=(2024, 2024),
            no_resize_control=True,
            model_type="flux2_klein_base_4b"
        )
    """
    import toml
    
    # Auto-detect settings based on model_type
    is_flux2 = model_type.startswith("flux2_")
    is_edit = model_type in ["qwen_image_edit", "qwen_image_edit_2509"]
    
    # Set default control_resolution based on model
    if control_dir and control_resolution is None:
        if is_edit:
            control_resolution = (1024, 1024)  # Recommended for Qwen Edit
        elif is_flux2:
            control_resolution = (2024, 2024)  # Single control for FLUX.2
    
    datasets = []
    
    # Qu√©t subfolders
    try:
        subfolders = sorted([f.path for f in os.scandir(data_dir) if f.is_dir()])
    except FileNotFoundError:
        subfolders = []
    
    # Th√™m root dir n·∫øu c√≥ ·∫£nh
    paths = subfolders.copy()
    if find_images_in_folder(data_dir):
        paths.insert(0, data_dir)
    
    for idx, folder_path in enumerate(paths):
        if not find_images_in_folder(folder_path):
            continue
        
        folder_name = os.path.basename(folder_path)
        if folder_path == data_dir:
            repeats = default_repeats
        else:
            repeats = get_repeats_from_folder_name(folder_name, default_repeats)
        
        # T·∫°o dataset entry
        dataset_item = {
            "image_directory": folder_path,
            "num_repeats": repeats
        }
        
        # Cache directory (kh√°c cho m·ªói dataset)
        if cache_dir:
            dataset_cache = os.path.join(cache_dir, f"cache_{idx}")
            dataset_item["cache_directory"] = dataset_cache
        
        # Control directory (cho Edit mode ho·∫∑c FLUX.2)
        if control_dir and (is_edit or is_flux2):
            dataset_item["control_directory"] = control_dir
            
            if control_resolution:
                dataset_item["control_resolution"] = list(control_resolution)
            
            if no_resize_control:
                dataset_item["no_resize_control"] = True
        
        print(f"   ‚úÖ Dataset: '{folder_name}' | Repeats: {repeats}")
        datasets.append(dataset_item)
    
    if not datasets:
        raise ValueError("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ·∫£nh n√†o!")
    
    # Build full config
    full_config = {
        "general": {
            "resolution": list(resolution),
            "caption_extension": caption_extension,
            "batch_size": batch_size,
            "enable_bucket": enable_bucket,
            "bucket_no_upscale": bucket_no_upscale,
        },
        "datasets": datasets
    }
    
    os.makedirs(config_dir, exist_ok=True)
    config_path = os.path.join(config_dir, "dataset_config.toml")
    
    # Write TOML v·ªõi format ƒë√∫ng
    with open(config_path, "w", encoding='utf-8') as f:
        # Write general section first
        f.write("[general]\n")
        for key, value in full_config["general"].items():
            if isinstance(value, bool):
                f.write(f"{key} = {str(value).lower()}\n")
            elif isinstance(value, list):
                f.write(f"{key} = {value}\n")
            elif isinstance(value, str):
                f.write(f'{key} = "{value}"\n')
            else:
                f.write(f"{key} = {value}\n")
        
        f.write("\n")
        
        # Write each dataset
        for ds in datasets:
            f.write("[[datasets]]\n")
            for key, value in ds.items():
                if isinstance(value, bool):
                    f.write(f"{key} = {str(value).lower()}\n")
                elif isinstance(value, list):
                    f.write(f"{key} = {value}\n")
                elif isinstance(value, str):
                    f.write(f'{key} = "{value}"\n')
                else:
                    f.write(f"{key} = {value}\n")
            f.write("\n")
    
    print("-" * 60)
    print(f"‚úÖ ƒê√£ t·∫°o: {config_path}")
    return config_path


# ==============================================================================
# SAMPLE GENERATION HELPERS
# ==============================================================================

def generate_sample_prompts(
    dataset_config_path: str,
    output_path: str,
    samples_per_dataset: int = 2,
    sample_width: int = 1024,
    sample_height: int = 1024,
    sample_steps: int = 28,
    control_dir: Optional[str] = None
) -> str:
    """
    T·∫°o file sample prompts t·ª´ dataset config.
    
    Args:
        dataset_config_path: Path ƒë·∫øn dataset_config.toml
        output_path: Path l∆∞u file samples.txt
        samples_per_dataset: S·ªë samples m·ªói dataset
        sample_width/height: K√≠ch th∆∞·ªõc sample
        sample_steps: S·ªë b∆∞·ªõc sampling
        control_dir: Th∆∞ m·ª•c control images (n·∫øu c·∫ßn)
    
    Returns:
        str: Path ƒë·∫øn file samples.txt
    """
    import toml
    import glob
    import random
    
    with open(dataset_config_path, "r", encoding='utf-8') as f:
        config_data = toml.load(f)
    
    datasets = config_data.get('datasets', [])
    lines = []
    
    for ds in datasets:
        img_dir = ds.get('image_directory')
        ctrl_dir = ds.get('control_directory', control_dir)
        
        if not img_dir or not os.path.exists(img_dir):
            continue
        
        images = []
        for ext in ('*.jpg', '*.jpeg', '*.png', '*.webp'):
            images.extend(glob.glob(os.path.join(img_dir, ext)))
        
        if images:
            selected = random.sample(images, min(samples_per_dataset, len(images)))
            for img in selected:
                cap_path = os.path.splitext(img)[0] + ".txt"
                caption = ""
                if os.path.exists(cap_path):
                    with open(cap_path, 'r', encoding='utf-8') as f:
                        caption = f.read().strip()
                
                if caption:
                    sample_line = f"{caption} --w {sample_width} --h {sample_height} --d {sample_steps}"
                    
                    # Th√™m control image n·∫øu c√≥
                    if ctrl_dir:
                        basename = os.path.splitext(os.path.basename(img))[0]
                        ctrl_candidates = glob.glob(os.path.join(ctrl_dir, f"{basename}.*"))
                        if ctrl_candidates:
                            sample_line += f" --ci {ctrl_candidates[0]}"
                    
                    lines.append(sample_line)
    
    if lines:
        with open(output_path, "w", encoding='utf-8') as f:
            f.write("\n".join(lines))
        print(f"‚úÖ T·∫°o {len(lines)} sample prompts: {output_path}")
    
    return output_path


# ==============================================================================
# COLAB STORE HELPERS
# ==============================================================================

def store_variables(var_dict: Dict[str, Any]):
    """
    L∆∞u bi·∫øn v√†o IPython %store (ch·∫°y trong Colab).
    
    Args:
        var_dict: Dict {t√™n_bi·∫øn: gi√°_tr·ªã}
    """
    try:
        from IPython import get_ipython
        from IPython.utils import capture
        
        ipython = get_ipython()
        if ipython is None:
            return
        
        g = ipython.user_ns
        for name, value in var_dict.items():
            g[name] = value
        
        with capture.capture_output():
            for name in var_dict.keys():
                ipython.run_line_magic('store', name)
        
        print(f"‚úÖ ƒê√£ l∆∞u {len(var_dict)} bi·∫øn")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ store bi·∫øn: {e}")


def restore_variables(*var_names: str) -> Dict[str, Any]:
    """
    Kh√¥i ph·ª•c bi·∫øn t·ª´ IPython %store.
    
    Args:
        var_names: T√™n c√°c bi·∫øn c·∫ßn kh√¥i ph·ª•c
    
    Returns:
        dict: {t√™n_bi·∫øn: gi√°_tr·ªã}
    """
    try:
        from IPython import get_ipython
        from IPython.utils import capture
        
        ipython = get_ipython()
        if ipython is None:
            return {}
        
        with capture.capture_output():
            for name in var_names:
                ipython.run_line_magic('store', f'-r {name}')
        
        result = {}
        for name in var_names:
            if name in ipython.user_ns:
                result[name] = ipython.user_ns[name]
        
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ restore bi·∫øn: {e}")
        return {}


# ==============================================================================
# MISC HELPERS
# ==============================================================================

def count_images_in_folder(folder: str) -> int:
    """ƒê·∫øm s·ªë ·∫£nh trong folder."""
    import glob
    count = 0
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        count += len(glob.glob(os.path.join(folder, ext)))
    return count


def get_all_image_paths(folder: str, recursive: bool = False) -> List[str]:
    """L·∫•y t·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n ·∫£nh trong folder."""
    import glob
    images = []
    pattern = "**/" if recursive else ""
    for ext in ['jpg', 'jpeg', 'png', 'webp', 'bmp']:
        images.extend(glob.glob(os.path.join(folder, f"{pattern}*.{ext}"), recursive=recursive))
    return sorted(images)


def ensure_dir(path: str) -> str:
    """T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i v√† tr·∫£ v·ªÅ path."""
    os.makedirs(path, exist_ok=True)
    return path


def read_caption(image_path: str, extension: str = ".txt") -> str:
    """ƒê·ªçc caption file t∆∞∆°ng ·ª©ng v·ªõi ·∫£nh."""
    cap_path = os.path.splitext(image_path)[0] + extension
    if os.path.exists(cap_path):
        with open(cap_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return ""


def write_caption(image_path: str, caption: str, extension: str = ".txt"):
    """Ghi caption file t∆∞∆°ng ·ª©ng v·ªõi ·∫£nh."""
    cap_path = os.path.splitext(image_path)[0] + extension
    with open(cap_path, 'w', encoding='utf-8') as f:
        f.write(caption)

